{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b898cefe",
   "metadata": {},
   "source": [
    "# 0. Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cf3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from datasetclasses import MeanVideoFramesDataset  # Dataset created specifically to calculate normalization values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ad91a",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mean_std(root, train_val_file, transform, only_normal):\n",
    "    dataset = MeanVideoFramesDataset(root, train_val_file, transform, only_normal)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loader = torch.utils.data.DataLoader(dataset,batch_size=128,shuffle=False,num_workers=24,pin_memory=True)\n",
    "\n",
    "    # Initialize accumulators\n",
    "    mean = torch.zeros(dataset[0].size(0)).to(device)  # Auto-detect number of channels\n",
    "    std = torch.zeros(dataset[0].size(0)).to(device)\n",
    "    total_samples = 0\n",
    "\n",
    "    for imgs in tqdm(loader):\n",
    "        # imgs shape: [B, C, H, W]\n",
    "        imgs = imgs.to(device)\n",
    "        batch_samples = imgs.size(0)\n",
    "        \n",
    "        # Reshape to [B, C, H*W]\n",
    "        imgs = imgs.view(batch_samples, imgs.size(1), -1)\n",
    "        \n",
    "        # Calculate mean and std per image, then sum across batch\n",
    "        mean += imgs.mean(dim=2).sum(dim=0)  # Sum across batch -> [C]\n",
    "        std += imgs.std(dim=2).sum(dim=0)    # Sum across batch -> [C]\n",
    "        \n",
    "        total_samples += batch_samples\n",
    "\n",
    "        # Average across all samples\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    print(\"Mean:\", mean.cpu().numpy())\n",
    "    print(\"Std :\", std.cpu().numpy())\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ac1b5",
   "metadata": {},
   "source": [
    "# 1. Metrics\n",
    "\n",
    "### Approach 1: Hasan et al (2017) uses 227x227x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b113ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2248/2248 [03:51<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FINAL DATASET NORMALIZATION VALUES ====\n",
      "Mean: [0.4200925  0.43748456 0.4467506 ]\n",
      "Std : [0.2505051  0.25867647 0.2695316 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = \"/home/public/mkamal/datasets/deep_learning/projdata/uploaded_data\"\n",
    "train_val_file = \"train_val.txt\"\n",
    "\n",
    "conv_grayscale_tranform227x227 = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "calculate_mean_std(root, train_val_file, conv_grayscale_tranform227x227, True)\n",
    "\n",
    "# (1x227x227) Grayscale Normal Normalized on train_val.txt\n",
    "conv_grayscale_tranform227x227 = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.43332753], [0.25522682])\n",
    "])\n",
    "\n",
    "# (3x227x227) RGB Normal Normalized on train_val.txt\n",
    "conv_RGB_tranform_227x227 = transforms.Compose([\n",
    "    transforms.Resize((227, 227)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4200925, 0.43748456, 0.4467506 ], [0.2505051, 0.25867647, 0.2695316 ])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb1eb3",
   "metadata": {},
   "source": [
    "==== (1x227x227) normal images Train_val.txt Noramlization ==== <br>\n",
    "Mean: [0.43332753] <br>\n",
    "Std : [0.25522682] <br><br>\n",
    "\n",
    "==== (3x227x227) normal images Train_val.txt Noramlization ==== <br>\n",
    "Mean: [0.4200925, 0.43748456, 0.4467506 ] <br>\n",
    "Std :  [0.2505051, 0.25867647, 0.2695316 ] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff3598",
   "metadata": {},
   "source": [
    "## Aproach 2: ViViT  Video Vision Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
