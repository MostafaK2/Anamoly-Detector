{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b898cefe",
   "metadata": {},
   "source": [
    "# 0. Important Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cf3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from datasetclasses import MeanVideoFramesDataset  # Dataset created specifically to calculate normalization values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ad91a",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9553efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mean_std(root, train_val_file, transform, only_normal):\n",
    "    dataset = MeanVideoFramesDataset(root, train_val_file, transform, only_normal)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loader = torch.utils.data.DataLoader(dataset,batch_size=128,shuffle=False,num_workers=24,pin_memory=True)\n",
    "\n",
    "    # Initialize accumulators\n",
    "    mean = torch.zeros(dataset[0].size(0)).to(device)  # Auto-detect number of channels\n",
    "    std = torch.zeros(dataset[0].size(0)).to(device)\n",
    "    total_samples = 0\n",
    "\n",
    "    for imgs in tqdm(loader):\n",
    "        # imgs shape: [B, C, H, W]\n",
    "        imgs = imgs.to(device)\n",
    "        batch_samples = imgs.size(0)\n",
    "        \n",
    "        # Reshape to [B, C, H*W]\n",
    "        imgs = imgs.view(batch_samples, imgs.size(1), -1)\n",
    "        \n",
    "        # Calculate mean and std per image, then sum across batch\n",
    "        mean += imgs.mean(dim=2).sum(dim=0)  # Sum across batch -> [C]\n",
    "        std += imgs.std(dim=2).sum(dim=0)    # Sum across batch -> [C]\n",
    "        \n",
    "        total_samples += batch_samples\n",
    "\n",
    "        # Average across all samples\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    print(\"Mean:\", mean.cpu().numpy())\n",
    "    print(\"Std :\", std.cpu().numpy())\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ac1b5",
   "metadata": {},
   "source": [
    "# 1. Metrics\n",
    "\n",
    "### Approach 1: Use Gray Scale or RGB Frames of Size 204x204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b113ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2248/2248 [04:01<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.42010325 0.43749467 0.44676054]\n",
      "Std : [0.249865   0.2580571  0.26895407]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ToTensor.__init__() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m calculate_mean_std(root, train_val_file, conv_grayscale_tranform204x204, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# (1x204x204) Grayscale Normal Normalized on train_val.txt\u001b[39;00m\n\u001b[32m     12\u001b[39m conv_grayscale_tranform204x204 = transforms.Compose([\n\u001b[32m     13\u001b[39m     transforms.Resize((\u001b[32m204\u001b[39m, \u001b[32m204\u001b[39m)),\n\u001b[32m     14\u001b[39m     transforms.Grayscale(num_output_channels=\u001b[32m1\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.43333843\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.25460896\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m ])\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# (1x204x204) Grayscale Normal Normalized on train_val.txt\u001b[39;00m\n\u001b[32m     19\u001b[39m conv_grayscale_tranform204x204 = transforms.Compose([\n\u001b[32m     20\u001b[39m     transforms.Resize((\u001b[32m204\u001b[39m, \u001b[32m204\u001b[39m)),\n\u001b[32m     21\u001b[39m     transforms.ToTensor([])\n\u001b[32m     22\u001b[39m ])\n",
      "\u001b[31mTypeError\u001b[39m: ToTensor.__init__() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "root = \"/home/public/mkamal/datasets/deep_learning/projdata/uploaded_data\"\n",
    "train_val_file = \"train_val.txt\"\n",
    "\n",
    "conv_grayscale_tranform204x204 = transforms.Compose([\n",
    "    transforms.Resize((204, 204)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "calculate_mean_std(root, train_val_file, conv_grayscale_tranform204x204, True)\n",
    "\n",
    "# (1x204x204) Grayscale Normal Normalized on train_val.txt\n",
    "conv_grayscale_tranform204x204 = transforms.Compose([\n",
    "    transforms.Resize((204, 204)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.43333843], [0.25460896])\n",
    "])\n",
    "\n",
    "# (3x204x204) Grayscale Normal Normalized on train_val.txt\n",
    "conv_RGB_tranform204x204 = transforms.Compose([\n",
    "    transforms.Resize((204, 204)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.42010325, 0.43749467, 0.44676054], [0.249865 , 0.2580571 ,0.26895407])\n",
    "])\n",
    "\n",
    "# (1x227x227) Grayscale Normal Normalized on train_val.txt\n",
    "conv_grayscale_tranform227x227 = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.43332753], [0.25522682])\n",
    "])\n",
    "\n",
    "# (3x227x227) RGB Normal Normalized on train_val.txt\n",
    "conv_RGB_tranform_227x227 = transforms.Compose([\n",
    "    transforms.Resize((227, 227)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4200925, 0.43748456, 0.4467506 ], [0.2505051, 0.25867647, 0.2695316 ])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb1eb3",
   "metadata": {},
   "source": [
    "==== (1x227x227) normal images Train_val.txt Noramlization ==== <br>\n",
    "Mean: [0.43332753] <br>\n",
    "Std : [0.25522682] <br><br>\n",
    "\n",
    "==== (3x227x227) normal images Train_val.txt Noramlization ==== <br>\n",
    "Mean: [0.4200925, 0.43748456, 0.4467506 ] <br>\n",
    "Std :  [0.2505051, 0.25867647, 0.2695316 ] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff3598",
   "metadata": {},
   "source": [
    "## Aproach 2: ViViT  Video Vision Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
